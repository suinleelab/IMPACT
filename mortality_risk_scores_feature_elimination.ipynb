{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4231c297-0e2e-4a78-9ca9-8a3a04a02894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import shap\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import argparse\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6445bc-6a33-418c-af3f-158ab997e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(permth, mortstat, month):\n",
    "    if permth > month:\n",
    "        return 0\n",
    "    else:\n",
    "        if mortstat == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9fccefc-b206-4d96-a125-fdd4ac45fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(y_pre, y_label, sample_size, repetitions = 1000, alpha = 0.05): \n",
    "    y_pre = np.array(y_pre)\n",
    "    y_label = np.array(y_label)\n",
    "    \n",
    "    auc = []\n",
    "    ap = []\n",
    "    for i in range(repetitions):\n",
    "        np.random.seed(i)\n",
    "        idx = list(np.random.choice(len(y_pre), replace = True, size = sample_size))\n",
    "        y_pre_bootstrap = y_pre[idx]\n",
    "        y_label_bootstrap = y_label[idx]\n",
    "        auc.append(roc_auc_score(y_label_bootstrap, y_pre_bootstrap))\n",
    "        ap.append(average_precision_score(y_label_bootstrap, y_pre_bootstrap))\n",
    "    # confidence interval\n",
    "    left_auc = np.percentile(auc, alpha/2*100)\n",
    "    right_auc = np.percentile(auc, 100-alpha/2*100)\n",
    "    left_ap = np.percentile(ap, alpha/2*100)\n",
    "    right_ap = np.percentile(ap, 100-alpha/2*100)\n",
    "    # point estimate\n",
    "    print('average AUROC', np.mean(auc))\n",
    "    print((1-alpha)*100,'%','confidence interval for the AUROC:', (round(left_auc,4), round(right_auc,4)))\n",
    "    print('average AP', np.mean(ap))\n",
    "    print((1-alpha)*100,'%','confidence interval for the AP:', (round(left_ap,4), round(right_ap,4)))\n",
    "    return auc, left_auc, right_auc, ap, left_ap, right_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "330c22e2-3249-4ed1-8b90-75ba40bc7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_num = 5\n",
    "path = './model/mortality_risk_scores_feature_elimination/'\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "448788b6-50ea-48f9-a269-1737e98f0372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Questionnaire_SelfReportedWeight10YrAgo',\n",
      "       'Laboratory_WhiteBloodCellCount', 'Laboratory_Cotinine',\n",
      "       'Laboratory_Sodium', 'Examination_BPDiastolic3',\n",
      "       'Laboratory_MeanCellVolume', 'Questionnaire_SelfReportedGreatestWeight',\n",
      "       'Laboratory_CholesterolSI', 'Examination_ExBPMaxInflationLevel',\n",
      "       'Laboratory_Monocyte',\n",
      "       ...\n",
      "       'Examination_BPReading3_2.0', 'Demographics_RaceEthnicity_2.0',\n",
      "       'Demographics_RaceEthnicity_3.0', 'Demographics_RaceEthnicity_4.0',\n",
      "       'Demographics_RaceEthnicity_5.0', 'Demographics_Gender_2.0',\n",
      "       'Questionnaire_MentalHealthProfessional_2.0',\n",
      "       'Questionnaire_DoctorCongestiveHeartFailure_2.0',\n",
      "       'Questionnaire_HighBloodPressure_2.0',\n",
      "       'Questionnaire_LabDietarySupplement_2.0'],\n",
      "      dtype='object', length=151)\n",
      "After encoding (35854, 151)\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('./data/NHANES/NHANES.csv')\n",
    "\n",
    "if str(year_num)+'_year_label' not in X.columns:\n",
    "    X[str(year_num)+'_year_label'] = X.apply(lambda x: label(x['permth_int'], x['mortstat'], 12*int(year_num)), axis=1)\n",
    "    \n",
    "X = X[X[str(year_num)+'_year_label']!=2]\n",
    "y = X[str(year_num)+'_year_label']\n",
    "\n",
    "if int(year_num) not in [1,2,3,4,5]:\n",
    "    X = X.drop([str(year_num)+'_year_label'], axis=1)\n",
    "\n",
    "mortstat = X['mortstat']\n",
    "permth_int = X['permth_int']\n",
    "drop_list = [\"mortstat\", \"permth_int\", '1_year_label', '2_year_label', '3_year_label', '4_year_label', '5_year_label']\n",
    "X = X.drop(drop_list, axis=1)\n",
    "X = X.drop(['Demographics_ReleaseCycle'], axis=1)\n",
    "fea_list = pd.read_csv('./data/NHANES/NHANES_feature_list.csv')\n",
    "nominal_fea = fea_list[fea_list['Nominal']==1]['Type_Short_Name'].tolist()\n",
    "nominal_fea = list(set(nominal_fea) & set(X.columns))\n",
    "X = pd.get_dummies(X, columns=nominal_fea, drop_first=True)\n",
    "print(X.columns)\n",
    "print('After encoding', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16ea3d4f-e337-4ef3-bcbf-10b96b2f237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Questionnaire_SelfReportedWeight10YrAgo',\n",
      "       'Laboratory_WhiteBloodCellCount', 'Laboratory_Cotinine',\n",
      "       'Laboratory_Sodium', 'Examination_BPDiastolic3',\n",
      "       'Laboratory_MeanCellVolume', 'Questionnaire_SelfReportedGreatestWeight',\n",
      "       'Laboratory_CholesterolSI', 'Examination_ExBPMaxInflationLevel',\n",
      "       'Laboratory_Monocyte',\n",
      "       ...\n",
      "       'Examination_BPReading3_2.0', 'Demographics_RaceEthnicity_2.0',\n",
      "       'Demographics_RaceEthnicity_3.0', 'Demographics_RaceEthnicity_4.0',\n",
      "       'Demographics_RaceEthnicity_5.0', 'Demographics_Gender_2.0',\n",
      "       'Questionnaire_MentalHealthProfessional_2.0',\n",
      "       'Questionnaire_DoctorCongestiveHeartFailure_2.0',\n",
      "       'Questionnaire_HighBloodPressure_2.0',\n",
      "       'Questionnaire_LabDietarySupplement_2.0'],\n",
      "      dtype='object', length=151)\n",
      "(35854, 151)\n",
      "# samples:  35854\n",
      "# positive samples:  3074\n",
      "# negative samples:  32780\n",
      "# features:  151\n"
     ]
    }
   ],
   "source": [
    "print(X.columns)\n",
    "print(X.shape)\n",
    "print('# samples: ', X.shape[0])\n",
    "print('# positive samples: ', sum(y==1))\n",
    "print('# negative samples: ', sum(y==0))\n",
    "print('# features: ', X.shape[1])  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=7)\n",
    "\n",
    "y_train = np.array(y_train); y_test = np.array(y_test); y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00edab62-e141-4283-afbe-e54257c101e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = pd.read_csv('./data/NHANES/NHANES_feature_list_Display_name.csv')\n",
    "display_col=[]\n",
    "for col in X.columns:\n",
    "    display_col.append(list(display_name.loc[display_name['Type_Short_Name']==col, 'Display_Name'])[0])\n",
    "col_dict = dict(zip(X.columns, display_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c7cb5a8-af43-4444-bca1-0b5b0b12b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num_list = [X_train.shape[1], 145, 140, 135, 130, 125, 120, 115, 110, 105, 100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 50, 45, 40, 35, 30, 25, 20, 15, 10,9,8,7,6,5,4,3,2,1]\n",
    "X_train_all = X_train.copy()\n",
    "X_test_all = X_test.copy()\n",
    "X_val_all = X_val.copy()\n",
    "\n",
    "features_ranking_dict = {}\n",
    "auc_dict = {}\n",
    "ap_dict = {}\n",
    "left_auc_dict = {}\n",
    "right_auc_dict = {}\n",
    "left_ap_dict = {}\n",
    "right_ap_dict = {}\n",
    "ranked_features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c309a9-01fb-42e8-a093-d05e7e61f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_input = {}\n",
    "ranked_features = X_train_all.columns\n",
    "for feature_num in feature_num_list:\n",
    "    print('# features: ', feature_num)\n",
    "    X_train = X_train_all.loc[:, ranked_features[:feature_num]]\n",
    "    X_test = X_test_all.loc[:, ranked_features[:feature_num]]\n",
    "    X_val = X_val_all.loc[:, ranked_features[:feature_num]]\n",
    "    features_input[feature_num] = ranked_features[:feature_num]\n",
    "    if feature_num > 2:\n",
    "        xlf = xgboost.XGBClassifier(n_estimators=1000, max_depth=4, subsample=0.5, min_child_weight=3, objective='binary:logistic', random_state=7)\n",
    "        xlf.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "    else:\n",
    "        xlf = xgboost.XGBClassifier(n_estimators=1000, subsample=0.5, objective='binary:logistic', random_state=7)\n",
    "        xlf.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "    model_train = xlf\n",
    "    pickle.dump(model_train, open(path+\"model_\"+str(feature_num)+\".pickle.dat\", \"wb\"))\n",
    "    y_pre = model_train.predict_proba(X_test)[:, 1]\n",
    "    auc, left_auc, right_auc, ap, left_ap, right_ap = bootstrap_ci(y_pre, y_test, len(y_test), repetitions = 1000, alpha = 0.05)\n",
    "    auc_dict[feature_num] = auc\n",
    "    ap_dict[feature_num] = ap\n",
    "    left_auc_dict[feature_num] = left_auc\n",
    "    right_auc_dict[feature_num] = right_auc\n",
    "    left_ap_dict[feature_num] = left_ap\n",
    "    right_ap_dict[feature_num] = right_ap    \n",
    "    if len(X_train)>=5000:\n",
    "        back_data = X_train.sample(n=5000, random_state=428)\n",
    "    else:\n",
    "        back_data = X_train\n",
    "    if len(X_test)>=2000:\n",
    "        fore_data = X_test.sample(n=2000, random_state=528)\n",
    "        fore_data_label = pd.DataFrame(y_test).sample(n=2000, random_state=528)\n",
    "    else:\n",
    "        fore_data = X_test\n",
    "        fore_data_label = pd.DataFrame(y_test)\n",
    "\n",
    "    explainer = shap.TreeExplainer(model_train, data=back_data)\n",
    "    shap_values = explainer.shap_values(fore_data, check_additivity=False)\n",
    "    ranked_features = X_train.columns[np.argsort(-np.sum(np.abs(shap_values), axis=0))]\n",
    "    features_ranking_dict[feature_num] = ranked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d245cfb-0a2b-4544-81c5-84fb18364c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(auc_dict, open(path+\"auc_dict.pickle.dat\", \"wb\"))\n",
    "pickle.dump(ap_dict, open(path+\"ap_dict.pickle.dat\", \"wb\"))\n",
    "pickle.dump(left_auc_dict, open(path+\"left_auc_dict.pickle.dat\", \"wb\"))\n",
    "pickle.dump(right_auc_dict, open(path+\"right_auc_dict.pickle.dat\", \"wb\"))\n",
    "pickle.dump(left_ap_dict, open(path+\"left_ap_dict.pickle.dat\", \"wb\"))\n",
    "pickle.dump(right_ap_dict, open(path+\"right_ap_dict.pickle.dat\", \"wb\"))\n",
    "pickle.dump(features_ranking_dict, open(path+\"features_ranking_dict.pickle.dat\", \"wb\"))\n",
    "pickle.dump(features_input, open(path+\"features_input.pickle.dat\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
